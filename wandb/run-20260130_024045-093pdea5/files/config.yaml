_wandb:
    value:
        cli_version: 0.23.1
        e:
            dcn7xr705vv3nej18ih7b272b5t670bq:
                args:
                    - --config
                    - recipes/unir.yaml
                    - --dataset_name
                    - openai/gsm8k
                    - --dataset_config
                    - main
                    - --output_dir
                    - run/GSM8k-llama-backbone3b_reasoning1b
                    - --run_name
                    - GSM8k-llama-backbone3b_reasoning1b
                    - --ref_name_or_path
                    - Qwen/Qwen2.5-3B-Instruct
                    - --model_name_or_path
                    - Qwen/Qwen2.5-0.5B-Instruct
                    - --num_generations
                    - "8"
                    - --per_device_eval_batch_size
                    - "8"
                    - --per_device_train_batch_size
                    - "8"
                    - --max_completion_length
                    - "1024"
                    - --max_steps
                    - "1000"
                    - --save_steps
                    - "100"
                    - --beta
                    - "0.0"
                    - --system_prompt
                    - 'A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively. Your response should be in the following format: <think>\nYour reasoning here\n</think>\n<answer>\n answer here \n</answer>. The reasoning process Note that respond by English, NOT use other languages.'
                    - --reward_funcs
                    - rule_based_accuracy
                    - --reward_weights
                    - "1.0"
                codePath: src/unir/train.py
                codePathLocal: src/unir/train.py
                cpu_count: 96
                cpu_count_logical: 192
                cudaVersion: "12.8"
                disk:
                    /:
                        total: "30396777496576"
                        used: "1038081155072"
                email: juliancodaforno@gmail.com
                executable: /home/tamboli/miniconda3/envs/unir_fresh/bin/python3.12
                git:
                    commit: df58cc1ce3ff3e09ecc5d377baf09cceba2e0f5e
                    remote: https://github.com/arushirai1/UniR.git
                gpu: NVIDIA H200
                gpu_count: 1
                gpu_nvidia:
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "150754820096"
                      name: NVIDIA H200
                      uuid: GPU-388ef51e-7c2e-816f-826f-b66dffb538a6
                host: h200-012-249
                memory:
                    total: "2147443974144"
                os: Linux-6.12.46-66.121.amzn2023.x86_64-x86_64-with-glibc2.35
                program: /storage/home/tamboli/UniR/src/unir/train.py
                python: CPython 3.12.12
                root: /storage/home/tamboli/UniR
                slurm:
                    cluster_name: shared-aws-usw1-1
                    conf: /etc/slurm/slurm.conf
                    cpu_bind: quiet,mask_cpu:0x000000000001000000000000000000000001000000000000
                    cpu_bind_list: 0x000000000001000000000000000000000001000000000000
                    cpu_bind_type: 'mask_cpu:'
                    cpu_bind_verbose: quiet
                    cpus_on_node: "2"
                    distribution: cyclic
                    gpus_on_node: "1"
                    gtids: "0"
                    job_account: mrs_2
                    job_cpus_per_node: "1"
                    job_end_time: "1769741303"
                    job_gid: "656444"
                    job_group: tamboli
                    job_id: "517928"
                    job_name: bash
                    job_nodelist: h200-012-249
                    job_num_nodes: "1"
                    job_partition: h200
                    job_qos: h200_mrs_shared
                    job_start_time: "1769734103"
                    job_uid: "656444"
                    job_user: tamboli
                    jobid: "517928"
                    launch_node_ipaddr: 10.136.24.45
                    localid: "0"
                    mem_per_node: "32768"
                    mpi_type: pmix
                    nnodes: "1"
                    nodeid: "0"
                    nodelist: h200-012-249
                    nprocs: "1"
                    ntasks: "1"
                    oom_kill_step: "0"
                    pmix_mapping_serv: (vector,(0,1,1))
                    pmixp_abort_agent_port: "40429"
                    prio_process: "0"
                    procid: "0"
                    pty_port: "38011"
                    pty_win_col: "113"
                    pty_win_row: "53"
                    script_context: prolog_task
                    srun_comm_host: 10.136.24.45
                    srun_comm_port: "39217"
                    step_gpus: "4"
                    step_id: "0"
                    step_launcher_port: "39217"
                    step_nodelist: h200-012-249
                    step_num_nodes: "1"
                    step_num_tasks: "1"
                    step_tasks_per_node: "1"
                    stepid: "0"
                    submit_dir: /storage/home/tamboli/UniR
                    submit_host: tamboli-login-0
                    task_pid: "2723401"
                    tasks_per_node: "1"
                    topology_addr: spine-usw1-az6-1.h200.h200-012-249
                    topology_addr_pattern: switch.switch.node
                    umask: "0007"
                startedAt: "2026-01-30T02:40:45.031680Z"
                writerId: dcn7xr705vv3nej18ih7b272b5t670bq
        m: []
        python_version: 3.12.12
        t:
            "1":
                - 1
                - 11
                - 49
                - 51
                - 71
                - 84
                - 98
            "2":
                - 1
                - 11
                - 49
                - 51
                - 71
                - 84
                - 98
            "3":
                - 13
                - 16
            "4": 3.12.12
            "5": 0.23.1
            "6": 4.49.0
            "12": 0.23.1
            "13": linux-x86_64
training_args:
    value:
        accelerator_config:
            dispatch_batches: null
            even_batches: true
            gradient_accumulation_kwargs: null
            non_blocking: false
            split_batches: false
            use_seedable_sampler: true
        adafactor: false
        adam_beta1: 0.9
        adam_beta2: 0.999
        adam_epsilon: 1e-08
        auto_find_batch_size: false
        average_tokens_across_devices: false
        batch_eval_metrics: false
        benchmarks: []
        beta: 0
        bf16: true
        bf16_full_eval: false
        callbacks: []
        chat_template: null
        data_seed: null
        dataloader_drop_last: false
        dataloader_num_workers: 0
        dataloader_persistent_workers: false
        dataloader_pin_memory: true
        dataloader_prefetch_factor: null
        ddp_backend: null
        ddp_broadcast_buffers: null
        ddp_bucket_cap_mb: null
        ddp_find_unused_parameters: null
        ddp_timeout: 1800
        debug: []
        deepspeed: null
        disable_tqdm: false
        dispatch_batches: null
        do_eval: false
        do_predict: false
        do_sample: true
        do_train: false
        ds3_gather_for_generation: true
        epsilon: 0.2
        eval_accumulation_steps: null
        eval_delay: 0
        eval_do_concat_batches: true
        eval_on_start: false
        eval_steps: null
        eval_strategy: "no"
        eval_use_gather_object: false
        evaluation_strategy: null
        fp16: false
        fp16_backend: auto
        fp16_full_eval: false
        fp16_opt_level: O1
        fsdp: []
        fsdp_config:
            min_num_params: 0
            xla: false
            xla_fsdp_grad_ckpt: false
            xla_fsdp_v2: false
        fsdp_min_num_params: 0
        fsdp_transformer_layer_cls_to_wrap: null
        full_determinism: false
        gradient_accumulation_steps: 4
        gradient_checkpointing: true
        gradient_checkpointing_kwargs:
            use_reentrant: false
        greater_is_better: null
        group_by_length: false
        half_precision_backend: auto
        hub_always_push: false
        hub_model_id: UniR
        hub_model_revision: main
        hub_private_repo: null
        hub_strategy: every_save
        hub_token: <HUB_TOKEN>
        ignore_data_skip: false
        include_for_metrics: []
        include_inputs_for_metrics: false
        include_num_input_tokens_seen: false
        include_tokens_per_second: false
        jit_mode_eval: false
        label_names: null
        label_smoothing_factor: 0
        learning_rate: 1e-06
        length_column_name: length
        load_best_model_at_end: false
        local_rank: 1
        log_completions: true
        log_level: info
        log_level_replica: warning
        log_on_each_node: true
        logging_dir: run/GSM8k-llama-backbone3b_reasoning1b/runs/Jan30_02-40-44_h200-012-249
        logging_first_step: true
        logging_nan_inf_filter: true
        logging_steps: 1
        logging_strategy: steps
        lr_scheduler_kwargs:
            min_lr_rate: 0.1
        lr_scheduler_type: cosine_with_min_lr
        max_completion_length: 1024
        max_grad_norm: 1
        max_prompt_length: 512
        max_steps: 1000
        metric_for_best_model: null
        model_init_kwargs: null
        mp_parameters: ""
        neftune_noise_alpha: null
        no_cuda: false
        num_generations: 8
        num_iterations: 1
        num_train_epochs: 1
        optim: adamw_torch
        optim_args: null
        optim_target_modules: null
        output_dir: run/GSM8k-llama-backbone3b_reasoning1b
        overwrite_hub_revision: false
        overwrite_output_dir: true
        past_index: -1
        per_device_eval_batch_size: 8
        per_device_train_batch_size: 8
        per_gpu_eval_batch_size: null
        per_gpu_train_batch_size: null
        prediction_loss_only: false
        push_to_hub: false
        push_to_hub_model_id: null
        push_to_hub_organization: null
        push_to_hub_revision: false
        push_to_hub_token: <PUSH_TO_HUB_TOKEN>
        ray_scope: last
        ref_model_mixup_alpha: 0.6
        ref_model_sync_steps: 512
        remove_unused_columns: false
        report_to:
            - tensorboard
        restore_callback_states_from_checkpoint: false
        resume_from_checkpoint: null
        reward_weights:
            - 1
        run_name: GSM8k-llama-backbone3b_reasoning1b
        save_on_each_node: false
        save_only_model: false
        save_safetensors: true
        save_steps: 100
        save_strategy: steps
        save_total_limit: null
        seed: 42
        skip_memory_metrics: true
        split_batches: null
        sync_ref_model: false
        system_prompt: 'A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively. Your response should be in the following format: <think>\nYour reasoning here\n</think>\n<answer>\n answer here \n</answer>. The reasoning process Note that respond by English, NOT use other languages.'
        temperature: 0.7
        tf32: null
        torch_compile: false
        torch_compile_backend: null
        torch_compile_mode: null
        torch_empty_cache_steps: null
        torchdynamo: null
        tpu_metrics_debug: false
        tpu_num_cores: null
        use_cpu: false
        use_ipex: false
        use_legacy_prediction_loop: false
        use_liger_kernel: false
        use_mps_device: false
        use_vllm: false
        vllm_device: auto
        vllm_dtype: auto
        vllm_enable_prefix_caching: true
        vllm_gpu_memory_utilization: 0.6
        vllm_guided_decoding_regex: null
        vllm_max_model_len: null
        wandb_entity: null
        wandb_project: null
        warmup_ratio: 0.1
        warmup_steps: 0
        weight_decay: 0
